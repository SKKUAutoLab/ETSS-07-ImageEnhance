{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "YOLOv7ONNXandTRT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install --upgrade setuptools pip --user\n",
    "!pip install --ignore-installed PyYAML\n",
    "!pip install Pillow\n",
    "\n",
    "!pip install nvidia-pyindex\n",
    "!pip install --upgrade nvidia-tensorrt\n",
    "!pip install pycuda\n",
    "\n",
    "!pip install protobuf<4.21.3\n",
    "!pip install onnxruntime-gpu\n",
    "!pip install onnx>=1.9.0\n",
    "!pip install onnx-simplifier>=0.3.6 --user"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sSDOngglBk_O",
    "outputId": "1c0a184f-3287-4284-d8de-d1c6222a5289"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hQ5fNost-gZI",
    "outputId": "d54c4359-07f6-40be-d4dd-2e3cc63387c4"
   },
   "source": [
    "import sys\n",
    "import torch\n",
    "print(f\"Python version: {sys.version}, {sys.version_info} \")\n",
    "print(f\"Pytorch version: {torch.__version__} \")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "feCaRUEI-_Os",
    "outputId": "b105660f-3f7a-4674-f570-5deda27a96c8"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!# Download YOLOv7 code\n",
    "!git clone https://github.com/WongKinYiu/yolov7\n",
    "%cd yolov7\n",
    "!ls"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yfZALjuo-_Md",
    "outputId": "42b96832-4eba-420e-a00d-24abe218242b"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!# Download trained weights\n",
    "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eWlHa1NJ-_Jw",
    "outputId": "70b58de9-c5e3-4a3e-9e7f-93cf2dcfcc3d"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!python detect.py --weights ./yolov7-tiny.pt --conf 0.25 --img-size 640 --source inference/images/horses.jpg"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UX7u8eqj-_Hi",
    "outputId": "01eb26c8-7689-4fe0-bd03-58158e2428c2"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "Image.open('/content/yolov7/runs/detect/exp/horses.jpg')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "wZD-nZXX-_Ez",
    "outputId": "c258429b-5c44-466d-b31f-035a20b960ae"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# export temporary ONNX model for TensorRT converter\n",
    "!python export.py --weights ./yolov7-tiny.pt --grid --end2end --simplify --topk-all 100 --iou-thres 0.65 --conf-thres 0.35 --img-size 640 640\n",
    "!ls"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1w4lEAvN16Sm",
    "outputId": "6c85fdcf-b4fe-4135-816e-faf5ab3a2d68"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Download ONNX to TensorRT converter\n",
    "%cd ../\n",
    "!git clone https://github.com/Linaom1214/tensorrt-python.git"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HSaHB--k_V8h",
    "outputId": "0535dc9b-b34c-40a7-c002-c87120229d2d"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%cd tensorrt-python\n",
    "!ls"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9rH_HHdd_V5P",
    "outputId": "b2f2a9aa-31e7-4985-afc0-beedb3663799"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Export TensorRT-engine model \n",
    "!python export.py -o /content/yolov7/yolov7-tiny.onnx -e ./yolov7-tiny-nms.trt -p fp16"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nIjoHQE2_V2g",
    "outputId": "866626d0-4522-4ecb-8465-a831146fad97"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict,namedtuple"
   ],
   "metadata": {
    "id": "Q101zl7-_aHd"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "w = './yolov7-tiny-nms.trt'\n",
    "device = torch.device('cuda:0')\n",
    "img = cv2.imread('/content/yolov7/inference/images/horses.jpg')"
   ],
   "metadata": {
    "id": "f-DABSAOw4Ri"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Infer TensorRT Engine\n",
    "Binding = namedtuple('Binding', ('name', 'dtype', 'shape', 'data', 'ptr'))\n",
    "logger = trt.Logger(trt.Logger.INFO)\n",
    "trt.init_libnvinfer_plugins(logger, namespace=\"\")\n",
    "with open(w, 'rb') as f, trt.Runtime(logger) as runtime:\n",
    "    model = runtime.deserialize_cuda_engine(f.read())\n",
    "bindings = OrderedDict()\n",
    "for index in range(model.num_bindings):\n",
    "    name = model.get_binding_name(index)\n",
    "    dtype = trt.nptype(model.get_binding_dtype(index))\n",
    "    shape = tuple(model.get_binding_shape(index))\n",
    "    data = torch.from_numpy(np.empty(shape, dtype=np.dtype(dtype))).to(device)\n",
    "    bindings[name] = Binding(name, dtype, shape, data, int(data.data_ptr()))\n",
    "binding_addrs = OrderedDict((n, d.ptr) for n, d in bindings.items())\n",
    "context = model.create_execution_context()\n",
    "\n",
    "\n",
    "def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleup=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = im.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return im, r, (dw, dh)\n",
    "\n",
    "def postprocess(boxes,r,dwdh):\n",
    "    dwdh = torch.tensor(dwdh*2).to(boxes.device)\n",
    "    boxes -= dwdh\n",
    "    boxes /= r\n",
    "    return boxes\n",
    "\n",
    "names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', \n",
    "         'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', \n",
    "         'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', \n",
    "         'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', \n",
    "         'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', \n",
    "         'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', \n",
    "         'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', \n",
    "         'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', \n",
    "         'hair drier', 'toothbrush']\n",
    "colors = {name:[random.randint(0, 255) for _ in range(3)] for i,name in enumerate(names)}"
   ],
   "metadata": {
    "id": "kRqqsjDcmyNj"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "image = img.copy()\n",
    "image, ratio, dwdh = letterbox(image, auto=False)\n",
    "image = image.transpose((2, 0, 1))\n",
    "image = np.expand_dims(image, 0)\n",
    "image = np.ascontiguousarray(image)\n",
    "\n",
    "im = image.astype(np.float32)\n",
    "im.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tzGt5tP9nJs_",
    "outputId": "b5e4658f-8b25-4926-bf87-dced1f966fff"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "im = torch.from_numpy(im).to(device)\n",
    "im/=255\n",
    "im.shape\n",
    "\n",
    "# warmup for 10 times\n",
    "for _ in range(10):\n",
    "    tmp = torch.randn(1,3,640,640).to(device)\n",
    "    binding_addrs['images'] = int(tmp.data_ptr())\n",
    "    context.execute_v2(list(binding_addrs.values()))\n",
    "\n",
    "start = time.perf_counter()\n",
    "binding_addrs['images'] = int(im.data_ptr())\n",
    "context.execute_v2(list(binding_addrs.values()))\n",
    "print(f'Cost {time.perf_counter()-start} s')\n",
    "\n",
    "nums = bindings['num_dets'].data\n",
    "boxes = bindings['det_boxes'].data\n",
    "scores = bindings['det_scores'].data\n",
    "classes = bindings['det_classes'].data\n",
    "nums.shape,boxes.shape,scores.shape,classes.shape\n",
    "\n",
    "boxes = boxes[0,:nums[0][0]]\n",
    "scores = scores[0,:nums[0][0]]\n",
    "classes = classes[0,:nums[0][0]]\n",
    "\n",
    "for box,score,cl in zip(boxes,scores,classes):\n",
    "    box = postprocess(box,ratio,dwdh).round().int()\n",
    "    name = names[cl]\n",
    "    color = colors[name]\n",
    "    name += ' ' + str(round(float(score),3))\n",
    "    cv2.rectangle(img,box[:2].tolist(),box[2:].tolist(),color,2)\n",
    "    cv2.putText(img,name,(int(box[0]), int(box[1]) - 2),cv2.FONT_HERSHEY_SIMPLEX,0.75,color,thickness=2)\n",
    "\n",
    "Image.fromarray(img)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "id": "xv8UsDWvn9i4",
    "outputId": "b960358f-8993-4b84-c8c8-d169676a014a"
   },
   "execution_count": 16,
   "outputs": []
  }
 ]
}
