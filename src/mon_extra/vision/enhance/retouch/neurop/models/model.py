import logging
import time
from collections import defaultdict, OrderedDict

import torch
import torch.nn as nn
from models.loss import *
from models.networks import *
from thop import profile
from torch.nn.parallel import DataParallel

import mon


class InitModel(nn.Module):
    
    def __init__(self, opt, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # My modification
        self.training = False
        
        self.opt    = opt
        self.device = torch.device(opt["device"])
        self.name   = "neurop_initialization"
        net_opt     = opt["network_G"]
        self.netG   = Renderer(net_opt["in_nc"], net_opt["out_nc"], net_opt["base_nf"]).to(self.device)
        self.netG   = DataParallel(self.netG)
        # self.print_network()
        self.load() 
        self.netG.train()
        
        train_opt = opt["train"]
        self.criterion = nn.L1Loss().to(self.device)
        self.optimizer = torch.optim.Adam(
            self.netG.parameters(),
            lr           = train_opt["lr_G"],
            weight_decay = 0,
            betas        = (train_opt["beta1"], train_opt["beta2"])
        )
        self.log_dict = OrderedDict()
    
    def feed_data(self, data):
        self.A_ex     = data["A_ex"].to(self.device)
        self.A_bc     = data["A_bc"].to(self.device)
        self.A_vb     = data["A_vb"].to(self.device)

        self.B_ex     = data["B_ex"].to(self.device)
        self.B_bc     = data["B_bc"].to(self.device)
        self.B_vb     = data["B_vb"].to(self.device)

        self.val_ex   = data["val_ex"].to(self.device)
        self.val_bc   = data["val_bc"].to(self.device)
        self.val_vb   = data["val_vb"].to(self.device)
        
        self.rec_A_ex = None
        self.rec_A_bc = None
        self.rec_A_vb = None
        self.map_B_ex = None
        self.map_B_bc = None
        self.map_B_vb = None
    
    def optimize_parameters(self):
        self.optimizer.zero_grad()
        self.rec_A_ex, self.rec_A_bc, self.rec_A_vb,self.map_B_ex, self.map_B_bc, self.map_B_vb = (
            self.netG(self.A_ex, self.A_bc, self.A_vb, self.val_ex, self.val_bc, self.val_vb)
        )

        loss_unary_ex = self.criterion(self.rec_A_ex, self.A_ex)
        loss_unary_bc = self.criterion(self.rec_A_bc, self.A_bc)
        loss_unary_vb = self.criterion(self.rec_A_vb, self.A_vb)
        loss_pair_ex  = self.criterion(self.map_B_ex, self.B_ex)
        loss_pair_bc  = self.criterion(self.map_B_bc, self.B_bc)
        loss_pair_vb  = self.criterion(self.map_B_vb, self.B_vb)

        loss_unary    = loss_unary_ex + loss_unary_bc + loss_unary_vb
        loss_pair     = loss_pair_ex + loss_pair_bc + loss_pair_vb
        loss          = loss_unary + loss_pair

        loss.backward()

        self.log_dict["loss_ex"]    = loss_unary_ex.item() + loss_pair_ex.item()
        self.log_dict["loss_bc"]    = loss_unary_bc.item() + loss_pair_bc.item()
        self.log_dict["loss_vb"]    = loss_unary_vb.item() + loss_pair_vb.item()
        self.log_dict["loss_unary"] = loss_unary.item()
        self.log_dict["loss_pair"]  = loss_pair.item()
        self.log_dict["loss_total"] = loss.item()
        self.optimizer.step()    
    
    def test(self):
        self.netG.eval()
        with torch.no_grad():
            self.rec_A_ex, self.rec_A_bc, self.rec_A_vb,self.map_B_ex, self.map_B_bc, self.map_B_vb = \
                self.netG(self.A_ex,self.A_bc,self.A_vb,self.val_ex,self.val_bc,self.val_vb)
        self.netG.train()        
    
    def get_current_log(self):
        return self.log_dict
    
    def get_current_visuals(self):
        out_dict = OrderedDict()
        out_dict["map_B_ex"] = self.map_B_ex.detach()[0].float().cpu().numpy().transpose(1, 2, 0)
        out_dict["map_B_bc"] = self.map_B_bc.detach()[0].float().cpu().numpy().transpose(1, 2, 0)
        out_dict["map_B_vb"] = self.map_B_vb.detach()[0].float().cpu().numpy().transpose(1, 2, 0)

        out_dict["B_ex"]     = self.B_ex.detach()[0].float().cpu().numpy().transpose(1, 2, 0)
        out_dict["B_bc"]     = self.B_bc.detach()[0].float().cpu().numpy().transpose(1, 2, 0)
        out_dict["B_vb"]     = self.B_vb.detach()[0].float().cpu().numpy().transpose(1, 2, 0)

        out_dict["rec_A_ex"] = self.rec_A_ex.detach()[0].float().cpu().numpy().transpose(1, 2, 0)
        out_dict["rec_A_bc"] = self.rec_A_bc.detach()[0].float().cpu().numpy().transpose(1, 2, 0)
        out_dict["rec_A_vb"] = self.rec_A_vb.detach()[0].float().cpu().numpy().transpose(1, 2, 0)

        out_dict["A_ex"]     = self.A_ex.detach()[0].float().cpu().numpy().transpose(1, 2, 0)
        out_dict["A_bc"]     = self.A_bc.detach()[0].float().cpu().numpy().transpose(1, 2 ,0)
        out_dict["A_vb"]     = self.A_vb.detach()[0].float().cpu().numpy().transpose(1, 2, 0)
        
        return out_dict
    
    def print_network(self):
        s, n = self.get_network_description(self.netG)
        if isinstance(self.netG, nn.DataParallel):
            net_struc_str = "{} - {}".format(self.netG.__class__.__name__, self.netG.module.__class__.__name__)
        else:
            net_struc_str = "{}".format(self.netG.__class__.__name__)

        logger = logging.getLogger("base")
        logger.info("Network G structure: {}, with parameters: {:,d}".format(net_struc_str, n))
        logger.info(s)

    def get_current_learning_rate(self):
        return [param_group["lr"] for param_group in self.optimizer.param_groups]

    def get_network_description(self, network):
        """Get the string and total parameters of the network"""
        if isinstance(network, nn.DataParallel):
            network = network.module
        return str(network), sum(map(lambda x: x.numel(), network.parameters()))

    def save(self, iter_label, save_dir):
        network       = self.netG
        save_filename = "{}_{}.pth".format(iter_label, self.name)
        save_dir      = mon.Path(save_dir)
        save_path     = save_dir / save_filename
        if isinstance(network, nn.DataParallel):
            network = network.module
        state_dict = network.state_dict()
        for key, param in state_dict.items():
            state_dict[key] = param.cpu()
        torch.save(state_dict, save_path)

    def load(self):
        load_path_G = self.opt["path"]["pretrain_model_G"]
        strict      = self.opt["path"]["strict_load"]
        network     = self.netG

        if load_path_G is not None:
            logger.info("Loading model for G [{:s}] ...".format(load_path_G))

            if isinstance(network, nn.DataParallel):
                network = network.module
            load_net = torch.load(load_path, weights_only=True)
            load_net_clean = OrderedDict()  # remove unnecessary "module."
            for k, v in load_net.items():
                if k.startswith("module."):
                    load_net_clean[k[7:]] = v
                else:
                    load_net_clean[k] = v
            network.load_state_dict(load_net_clean, strict=strict)
    
    # My modification
    
    def forward(self):
        self.test()
    
    def measure_efficiency_score(self, image_size=512, channels=3, runs=1000):
        h, w  = mon.get_image_size(image_size)
        input = torch.rand(1, channels, h, w).cuda()
        data  = {
            "idx": 0,
            "LQ" : input,
            "GT" : input,
        }
        self.netG = self.netG.cuda()
        self.feed_data(data)
        flops, params = profile(self, inputs=(), verbose=False)
        g_flops       = flops  * 1e-9
        m_params      = params * 1e-6
        start_time    = time.time()
        for i in range(runs):
            self.forward()
        runtime  = time.time() - start_time
        avg_time = runtime / runs
        return flops, params, avg_time
    

class FinetuneModel(nn.Module):
    
    def __init__(self, opt, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # My modification
        self.training = False
        
        self.opt    = opt
        self.device = torch.device(opt["device"])
        self.name   = "neurop_"+opt["datasets"]["name"]
        net_opt     = opt["network_G"]
        self.netG   = NeurOP(net_opt["in_nc"], net_opt["out_nc"], net_opt["base_nf"], net_opt["cond_nf"], net_opt["init_model"])
        if opt["weights"] is not None:
            self.netG.load_state_dict(torch.load(opt["weights"], weights_only=True))
        self.netG   = self.netG.to(self.device)
        # self.netG = DataParallel(self.netG)
        # self.print_network()
        if opt["is_train"]:
            self.netG.train()
            train_opt = opt["train"]
            self.cri_pix   = nn.L1Loss().to(self.device)
            self.cri_cos   = CosineLoss().to(self.device)
            self.cri_tv    = TVLoss().to(self.device)
            self.cri_ratio = 1 / train_opt["pixel_weight"]
            self.optimizer = torch.optim.Adam(self.netG.parameters(), lr=train_opt["lr_G"], weight_decay=0, betas=(train_opt["beta1"], train_opt["beta2"]))
        self.log_dict = defaultdict(int)

    def feed_data(self, data):
        self.var_L  = data["LQ"].to(self.device)
        self.real_H = data["GT"].to(self.device)         
        self.fake_H = None
 
    def optimize_parameters(self):
        self.optimizer.zero_grad()
        self.fake_H = self.netG(img=self.var_L, return_vals=False)
        loss_pix    = self.cri_pix(self.fake_H, self.real_H)
        loss_cos    = self.cri_cos(self.fake_H, self.real_H)
        loss_tv     = self.cri_tv(self.fake_H)
        loss        = loss_pix + self.cri_ratio * (loss_cos + loss_tv)
        loss.backward()

        self.log_dict["loss_pix"]   += loss_pix.item()
        self.log_dict["loss_cos"]   += loss_cos.item() 
        self.log_dict["loss_tv"]    += loss_tv.item()
        self.log_dict["loss_total"] += loss.item()
        self.optimizer.step()    

    def test(self):
        self.netG.eval()
        with torch.no_grad():
            self.fake_H = self.netG(img=self.var_L, return_vals=False)
        self.netG.train()        

    def get_current_log(self):
        return self.log_dict

    def get_current_visuals(self):
        out_dict        = OrderedDict()
        out_dict["LQ"]  =  self.var_L.detach()[0].float().cpu().numpy().transpose(1, 2, 0)
        out_dict["rlt"] = self.fake_H.detach()[0].float().cpu().numpy().transpose(1, 2, 0)
        out_dict["GT"]  = self.real_H.detach()[0].float().cpu().numpy().transpose(1, 2, 0)
        return out_dict

    def print_network(self):
        s, n = self.get_network_description(self.netG)
        if isinstance(self.netG, nn.DataParallel):
            net_struc_str = "{} - {}".format(self.netG.__class__.__name__, self.netG.module.__class__.__name__)
        else:
            net_struc_str = "{}".format(self.netG.__class__.__name__)

        logger = logging.getLogger("base")
        logger.info("Network G structure: {}, with parameters: {:,d}".format(net_struc_str, n))
        logger.info(s)

    def get_current_learning_rate(self):
        return [param_group["lr"] for param_group in self.optimizer.param_groups]

    def get_network_description(self, network):
        """Get the string and total parameters of the network"""
        if isinstance(network, nn.DataParallel):
            network = network.module
        return str(network), sum(map(lambda x: x.numel(), network.parameters()))

    def save(self, iter_label, save_dir):
        network       = self.netG
        save_filename = "{}_{}.pth".format(iter_label, self.name)
        save_dir      = mon.Path(save_dir)
        save_path     = save_dir / save_filename
        if isinstance(network, nn.DataParallel):
            network = network.module
        state_dict = network.state_dict()
        for key, param in state_dict.items():
            state_dict[key] = param.cpu()
        torch.save(state_dict, save_path)
    
    # My modification
    
    def forward(self):
        self.test()
        
    def measure_efficiency_score(self, image_size=512, channels=3, runs=1000):
        h, w  = mon.get_image_size(image_size)
        input = torch.rand(1, channels, h, w).to(self.device)
        data  = {
            "idx": 0,
            "LQ" : input,
            "GT" : input,
        }
        self.feed_data(data)
        flops, params = profile(self, inputs=(), verbose=False)
        g_flops       = flops  * 1e-9
        m_params      = params * 1e-6
        start_time    = time.time()
        for i in range(runs):
            self.forward()
        runtime  = time.time() - start_time
        avg_time = runtime / runs
        return flops, params, avg_time
